{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabca1b9",
   "metadata": {},
   "source": [
    "# Creating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed19df1d",
   "metadata": {},
   "source": [
    "В данном разделе написан код, который создает датасет из спектограмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b71b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad02d9e",
   "metadata": {},
   "source": [
    "    y, sr = librosa.load(wav_file)\n",
    "    window = np.hanning(window_size)\n",
    "    stft  = librosa.core.spectrum.stft(y, n_fft=window_size, hop_length=512, window=window)\n",
    "    out = 2 * np.abs(stft) / np.sum(window)\n",
    "    canvas = FigureCanvas(fig)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axis('off')\n",
    "    p = librosa.display.specshow(librosa.amplitude_to_db(out, ref=np.max), ax=ax)\n",
    "    temp = wav_file.split('/')[-1]\n",
    "    temp = './' + temp.split('.')[0]+'.png'\n",
    "    \n",
    "    fig.savefig(temp\n",
    "                ,pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802038eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_data(audio_folder:str,img_folder:str,window_size=1024,hop_length=512):\n",
    "    '''\n",
    "    данная функция генерирует по датасету аудиофайлов датасет спектрограмм(изображений)\n",
    "    \n",
    "    ==Input==\n",
    "    audio_folder - путь к директории с датасетом аудиофайлов\n",
    "    img_folder - путь к директории куда складывать изображения\n",
    "    window_size - по сути размер скользящего окна\n",
    "    '''\n",
    "    if not os.path.exists(img_folder):\n",
    "        os.makedirs(img_folder)\n",
    "    # цикл по подпапкам и файлам\n",
    "    for subdir, dirs, files in os.walk(audio_folder):\n",
    "        print(subdir)\n",
    "        for file in tqdm(files):\n",
    "            filepath = subdir + os.sep + file\n",
    "            if filepath.endswith(\".wav\"):\n",
    "                fig = plt.Figure(frameon=False)\n",
    "                y, sr = librosa.load(filepath)\n",
    "                window = np.hanning(window_size) # построение скользящего окна для сглаживания\n",
    "                stft  = librosa.core.spectrum.stft(y,   # разложение в Фурье\n",
    "                                                   n_fft=window_size, \n",
    "                                                   hop_length=hop_length, \n",
    "                                                   window=window)\n",
    "                out = 2 * np.abs(stft) / np.sum(window)\n",
    "                canvas = FigureCanvas(fig)\n",
    "                ax = fig.add_subplot(111)\n",
    "                ax.axis('off')\n",
    "                p = librosa.display.specshow(librosa.amplitude_to_db(out,  # построение спектрограммы\n",
    "                                                                     ref=np.max),\n",
    "                                             ax=ax)\n",
    "                path=os.path.join(img_folder,subdir.split('/')[2])\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "                f_name=os.path.join(path,file.split('.')[0]+'.png')   \n",
    "                fig.savefig(f_name\n",
    "                            ,pad_inches = 0)\n",
    "                # закрытие и удаление объектов \"график\" чтобы не переполнялась память\n",
    "                fig.clear()\n",
    "                plt.close(fig)\n",
    "            gc.collect()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0497bbae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/street_music\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [07:28<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/air_conditioner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [07:25<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/jackhammer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [07:40<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/siren\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 929/929 [06:46<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/engine_idling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [07:40<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/gun_shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 374/374 [01:52<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/drilling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [07:22<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/car_horn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 429/429 [02:27<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/dog_bark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:39<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/segregated-urban8K-sounds/children_playing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [07:26<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 2min 36s, sys: 12 s, total: 1h 2min 48s\n",
      "Wall time: 1h 2min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_img_data('archive/segregated-urban8K-sounds','./img')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b271ee",
   "metadata": {},
   "source": [
    "# Spliting Dataset to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9973e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "267dac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(img_dir:str, dataset_dir:str,test_ratio = 0.20 ):\n",
    "    classes = os.listdir(img_dir)\n",
    "    for i in classes:\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.makedirs(dataset_dir)\n",
    "        if not os.path.exists(dataset_dir +'/train/' + i):\n",
    "            os.makedirs(dataset_dir +'/train/' + i)\n",
    "        if not os.path.exists(dataset_dir +'/val/' + i):\n",
    "            os.makedirs(dataset_dir +'/val/' + i)\n",
    "        source = img_dir + '/' + i\n",
    "        allFileNames = os.listdir(source)\n",
    "        np.random.shuffle(allFileNames)\n",
    "        \n",
    "        train_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                              [int(len(allFileNames)* (1 - test_ratio))])\n",
    "        train_FileNames = [source+'/'+ name for name in train_FileNames.tolist()]\n",
    "        test_FileNames = [source+'/' + name for name in test_FileNames.tolist()]\n",
    "        for name in train_FileNames:\n",
    "            shutil.copy(name, dataset_dir +'/train/' + i)\n",
    "        for name in test_FileNames:\n",
    "            shutil.copy(name, dataset_dir +'/val/' + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fae698c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 ms, sys: 1.06 s, total: 1.3 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "split_train_test('./img','img_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277dbb5e",
   "metadata": {},
   "source": [
    "# Creating classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d573d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec60c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TRAIN = \"./img_dataset/train\"\n",
    "DIR_VALID = \"./img_dataset/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e7ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Classes:  10\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(DIR_TRAIN)\n",
    "print(\"Total Classes: \",len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0de1f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street_music',\n",
       " 'air_conditioner',\n",
       " 'jackhammer',\n",
       " 'siren',\n",
       " 'engine_idling',\n",
       " 'gun_shot',\n",
       " 'drilling',\n",
       " 'car_horn',\n",
       " 'dog_bark',\n",
       " 'children_playing']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74bf63fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train images:  6985\n",
      "Total valid images:  1747\n"
     ]
    }
   ],
   "source": [
    "train_count = 0\n",
    "valid_count = 0\n",
    "\n",
    "for _class in classes:\n",
    "    train_count += len(os.listdir(DIR_TRAIN +'/'+ _class))\n",
    "    valid_count += len(os.listdir(DIR_VALID +'/'+ _class))\n",
    "print(\"Total train images: \",train_count)\n",
    "print(\"Total valid images: \",valid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f536de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем списки со всеми изображениями для тренировки и для валидации\n",
    "#\n",
    "#\n",
    "train_imgs = []\n",
    "valid_imgs = []\n",
    "for _class in classes:\n",
    "    \n",
    "    for img in os.listdir(DIR_TRAIN +'/'+ _class):\n",
    "        train_imgs.append(DIR_TRAIN +'/'+ _class + \"/\" + img)\n",
    "    \n",
    "    for img in os.listdir(DIR_VALID +'/'+ _class):\n",
    "        valid_imgs.append(DIR_VALID +'/'+ _class + \"/\" + img)\n",
    "        \n",
    "\n",
    "class_to_int = {classes[i] : i for i in range(len(classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90a97de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'street_music': 0,\n",
       " 'air_conditioner': 1,\n",
       " 'jackhammer': 2,\n",
       " 'siren': 3,\n",
       " 'engine_idling': 4,\n",
       " 'gun_shot': 5,\n",
       " 'drilling': 6,\n",
       " 'car_horn': 7,\n",
       " 'dog_bark': 8,\n",
       " 'children_playing': 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a2ecf",
   "metadata": {},
   "source": [
    "There are multiple ways to load images from the dataset, I have used 2 such methods:\n",
    "\n",
    "\n",
    "Using Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b31f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем класс датасета. Этот объект будет подавать изображения и метки классов в нейронку пачками\n",
    "# Пачками потому что нельзя все карьтнки разом загрузить в память и там работать.\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    return T.Compose([T.ToTensor()])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, imgs_list, class_to_int, transforms = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.imgs_list = imgs_list\n",
    "        self.class_to_int = class_to_int\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        image_path = self.imgs_list[index]\n",
    "        \n",
    "        #Reading image\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        #Retriving class label\n",
    "        label = image_path.split(\"/\")[-2]\n",
    "        label = self.class_to_int[label]\n",
    "        \n",
    "        #Applying transforms on image\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a465f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем экземпляры нашего класса датасет\n",
    "train_dataset = CustomDataset(train_imgs, class_to_int, get_transform())\n",
    "valid_dataset = CustomDataset(valid_imgs, class_to_int, get_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2de24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем Сэмплер. по сути он просто будет перемешивать идущие на вход картинки\n",
    "# чтобы при каждом цикле все шло в разном порядке. \n",
    "train_random_sampler = RandomSampler(train_dataset)\n",
    "valid_random_sampler = RandomSampler(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9e3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем экземпляры Загрузчика данных\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = 4,\n",
    "    sampler = train_random_sampler,\n",
    "    num_workers = 2,\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = 4,\n",
    "    sampler = valid_random_sampler,\n",
    "    num_workers = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просто показать картинки , ничего особенного\n",
    "for images, labels in train_data_loader:\n",
    "    fig, ax = plt.subplots(figsize = (10, 10))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(make_grid(images, 4).permute(1,2,0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9439ee",
   "metadata": {},
   "source": [
    "mobilenet_v3_large best loss 0.19127139026714768 best acc 100.0 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4154abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_model(num_classes):\n",
    "    '''\n",
    "    Эта функция возвращает экземпляр модели, построенной под наше количество классов.\n",
    "    Мы берем предобученную сеть, например mobilenet_v3_large и меняем ей \"голову\"\n",
    "    '''\n",
    "    model = models.mobilenet_v3_large(pretrained = True)\n",
    "    \n",
    "    #print(model)\n",
    "    \n",
    "    last_layer_input_features = model.classifier[-1].in_features\n",
    "    \n",
    "    model.classifier[-1] = nn.Linear(last_layer_input_features, num_classes, bias = True)\n",
    "    \n",
    "    #print(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c54f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=return_model(len(classes)) # создаем нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fdeed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "### Нюансы обучения\n",
    "#optimizer = torch.optim.Adam(model.classifier.parameters(), lr = 0.0001)\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr = 0.0001) # метод оптимизации\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.75)\n",
    "criterion = nn.CrossEntropyLoss() # функция потерь\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "\n",
    "val_loss = []\n",
    "val_accuracy = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f4a732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(true,pred):\n",
    "    pred = F.softmax(pred, dim = 1)\n",
    "    true = torch.zeros(pred.shape[0], pred.shape[1]).scatter_(1, true.unsqueeze(1), 1.)\n",
    "    acc = (true.argmax(-1) == pred.argmax(-1)).float().detach().numpy()\n",
    "    acc = float((100 * acc.sum()) / len(acc))\n",
    "    return round(acc, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb83900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f2399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Epoch 0 ** - Epoch Time 1101\n",
      "Train Loss = 1.486\n",
      "Train Accuracy = 48.82655981682885 % \n",
      "\n",
      "Val Loss = 1.2551\n",
      "Val Accuracy = 57.341723798627 % \n",
      "\n",
      "loss descrease 1.255055527733447< inf\n",
      "** Epoch 1 ** - Epoch Time 2794\n",
      "Train Loss = 1.1701\n",
      "Train Accuracy = 59.87406983400115 % \n",
      "\n",
      "Val Loss = 1.1873\n",
      "Val Accuracy = 58.46681922196797 % \n",
      "\n",
      "loss descrease 1.1873221753050043< 1.255055527733447\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Training Code\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "PATH = './best_result_10classes.pt'\n",
    "best_loss = math.inf\n",
    "train = True\n",
    "earley_stoping = 10\n",
    "earley_stoping_counter = 0\n",
    "epoch=0\n",
    "while train:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #Epoch Loss & Accuracy\n",
    "    train_epoch_loss = []\n",
    "    train_epoch_accuracy = []\n",
    "    \n",
    "    #Val Loss & Accuracy\n",
    "    val_epoch_loss = []\n",
    "    val_epoch_accuracy = []\n",
    "    \n",
    "    # Training\n",
    "    for images, labels in train_data_loader:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Reset Grads\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward ->\n",
    "        preds = model(images)\n",
    "        #Calculate Accuracy\n",
    "        acc = calc_accuracy(labels.cpu(), preds.cpu())\n",
    "        \n",
    "        #Calculate Loss & Backward, Update Weights (Step)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Append loss & acc\n",
    "        loss_value = loss.item()\n",
    "        train_epoch_loss.append(loss_value)\n",
    "        train_epoch_accuracy.append(acc)\n",
    "    \n",
    "    #Validation\n",
    "    for images, labels in valid_data_loader:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward ->\n",
    "        preds = model(images)\n",
    "        \n",
    "        #Calculate Accuracy\n",
    "        acc = calc_accuracy(labels.cpu(), preds.cpu())\n",
    "        \n",
    "        #Calculate Loss\n",
    "        loss = criterion(preds, labels)\n",
    "        \n",
    "        #Append loss & acc\n",
    "        loss_value = loss.item()\n",
    "        val_epoch_loss.append(loss_value)\n",
    "        val_epoch_accuracy.append(acc)\n",
    "    \n",
    "    \n",
    "    train_epoch_loss = np.mean(train_epoch_loss)\n",
    "    train_epoch_accuracy = np.mean(train_epoch_accuracy)\n",
    "    \n",
    "    val_epoch_loss = np.mean(val_epoch_loss)\n",
    "    val_epoch_accuracy = np.mean(val_epoch_accuracy)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_accuracy.append(train_epoch_accuracy)\n",
    "    \n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "    \n",
    "    #Print Epoch Statistics\n",
    "    print(\"** Epoch {} ** - Epoch Time {}\".format(epoch, int(end-start)))\n",
    "    print(\"Train Loss = {}\".format(round(train_epoch_loss, 4)))\n",
    "    print(\"Train Accuracy = {} % \\n\".format(train_epoch_accuracy))\n",
    "    print(\"Val Loss = {}\".format(round(val_epoch_loss, 4)))\n",
    "    print(\"Val Accuracy = {} % \\n\".format(val_epoch_accuracy))\n",
    "    epoch+=1\n",
    "    if val_epoch_loss< best_loss:\n",
    "        print(f'loss descrease {val_epoch_loss}< {best_loss}')\n",
    "        best_loss = val_epoch_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        earley_stoping_counter = 0\n",
    "    else:\n",
    "        earley_stoping_counter+=1\n",
    "        print(f'{earley_stoping_counter} epochs without increasing')\n",
    "    if earley_stoping_counter >= earley_stoping:\n",
    "        print(f'enaught')\n",
    "        train = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=train_val_acc_df, x = \"epochs\", y=\"value\", hue=\"variable\",\n",
    "             ax=axes[0]).set_title('Train-Val Accuracy/Epoch')\n",
    "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\",\n",
    "             ax=axes[1]).set_title('Train-Val Loss/Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5757956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d50e685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./venv/lib/python3.8/site-packages (from pandas) (1.21.5)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./venv/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.4.1 pytz-2022.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc44d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ee958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "#plt.rcParams[\"figure.figsize\"] = [6.40, 4.80]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "filename = './children_playing/6902-2-0-4.wav'\n",
    "y, sr = librosa.load(filename)\n",
    "#y = y[:100000] # shorten audio a bit for speed\n",
    "\n",
    "window_size = 1024\n",
    "window = np.hanning(window_size)\n",
    "stft  = librosa.core.spectrum.stft(y, n_fft=window_size, hop_length=512, window=window)\n",
    "out = 2 * np.abs(stft) / np.sum(window)\n",
    "\n",
    "# For plotting headlessly\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "fig = plt.Figure(frameon=False)\n",
    "canvas = FigureCanvas(fig)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis('off')\n",
    "p = librosa.display.specshow(librosa.amplitude_to_db(out, ref=np.max), ax=ax)\n",
    "fig.savefig('spec.png',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbeb3c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./venv/lib/python3.8/site-packages (9.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88c44f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e40a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = T.Compose([T.ToTensor()])\n",
    "\n",
    "model1 = return_model(5)\n",
    "model1.load_state_dict(torch.load('./best_result1.pt'))\n",
    "model1.eval()\n",
    "window_size = 1024\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def infer(wav_file, transforms= None):\n",
    "    \n",
    "    y, sr = librosa.load(wav_file)\n",
    "    window = np.hanning(window_size)\n",
    "    stft  = librosa.core.spectrum.stft(y, n_fft=window_size, hop_length=512, window=window)\n",
    "    out = 2 * np.abs(stft) / np.sum(window)\n",
    "    canvas = FigureCanvas(fig)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axis('off')\n",
    "    p = librosa.display.specshow(librosa.amplitude_to_db(out, ref=np.max), ax=ax)\n",
    "    temp = wav_file.split('/')[-1]\n",
    "    temp = './' + temp.split('.')[0]+'.png'\n",
    "    \n",
    "    fig.savefig(temp\n",
    "                ,pad_inches = 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    image = cv2.imread(temp, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image /= 255.0\n",
    "    #Applying transforms on image\n",
    "    \n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    #input = Variable(image_tensor)\n",
    "    image_tensor.to(device)\n",
    "    output = model1(image_tensor)\n",
    "    index = torch.argmax(output, dim=1)\n",
    "    #index = output.data.cpu().numpy().argmax()\n",
    "    \n",
    "    \n",
    "    #image = torch.tensor(image)\n",
    "    \n",
    "    #image = image.to(device)\n",
    "    #preds = model(images)\n",
    "    #index = output.data.cpu().numpy().argmax()\n",
    "    return index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5bec6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_int = {'air_conditioner': 0,\n",
    " 'drilling': 1,\n",
    " 'dog_bark': 2,\n",
    " 'children_playing': 3,\n",
    " 'car_horn': 4}\n",
    "int_to_class={v: k for k, v in class_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cd3beb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "children_playing\n"
     ]
    }
   ],
   "source": [
    "print(int_to_class[infer('./children_playing/6902-2-0-4.wav',get_transform).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241ddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6d2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6a344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d8030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda3046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d4702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32af1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaee056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0bad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5ca04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ead70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0ef91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221704a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
